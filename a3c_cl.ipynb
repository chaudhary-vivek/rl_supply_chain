{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3073977c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 2021\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class State:\n",
    "    \"\"\"State representation for supply chain environment\"\"\"\n",
    "    \n",
    "    def __init__(self, product_types_num, distr_warehouses_num, T, demand_history, t=0):\n",
    "        self.product_types_num = product_types_num\n",
    "        self.factory_stocks = np.zeros((self.product_types_num,), dtype=np.int32)\n",
    "        self.distr_warehouses_num = distr_warehouses_num\n",
    "        self.distr_warehouses_stocks = np.zeros(\n",
    "            (self.distr_warehouses_num, self.product_types_num), dtype=np.int32)\n",
    "        self.T = T\n",
    "        self.demand_history = demand_history\n",
    "        self.t = t\n",
    "\n",
    "    def to_array(self):\n",
    "        from itertools import chain\n",
    "        return np.concatenate((\n",
    "            self.factory_stocks,\n",
    "            self.distr_warehouses_stocks.flatten(),\n",
    "            np.hstack(list(chain(*chain(*self.demand_history)))),\n",
    "            [self.t]))\n",
    "\n",
    "    def stock_levels(self):\n",
    "        return np.concatenate((\n",
    "            self.factory_stocks,\n",
    "            self.distr_warehouses_stocks.flatten()))\n",
    "\n",
    "class Action:\n",
    "    \"\"\"Action representation for supply chain environment\"\"\"\n",
    "    \n",
    "    def __init__(self, product_types_num, distr_warehouses_num):\n",
    "        self.production_level = np.zeros((product_types_num,), dtype=np.int32)\n",
    "        self.shipped_stocks = np.zeros(\n",
    "            (distr_warehouses_num, product_types_num), dtype=np.int32)\n",
    "\n",
    "class SupplyChainEnvironment:\n",
    "    \"\"\"Supply chain environment implementation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.product_types_num = 2\n",
    "        self.distr_warehouses_num = 2\n",
    "        self.T = 25\n",
    "        \n",
    "        self.d_max = np.array([3, 6], np.int32)\n",
    "        self.d_var = np.array([2, 1], np.int32)\n",
    "        \n",
    "        self.sale_prices = np.array([20, 10], np.int32)\n",
    "        self.production_costs = np.array([2, 1], np.int32)\n",
    "        \n",
    "        self.storage_capacities = np.array([[3, 4], [6, 8], [9, 12]], np.int32)\n",
    "        self.storage_costs = np.array([[6, 3], [4, 2], [2, 1]], np.float32)\n",
    "        self.transportation_costs = np.array([[.1, .3], [.2, .6]], np.float32)\n",
    "        \n",
    "        self.penalty_costs = .5 * self.sale_prices\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, demand_history_len=5):\n",
    "        self.demand_history = collections.deque(maxlen=demand_history_len)\n",
    "        for d in range(demand_history_len):\n",
    "            self.demand_history.append(np.zeros(\n",
    "                (self.distr_warehouses_num, self.product_types_num), dtype=np.int32))\n",
    "        self.t = 0\n",
    "\n",
    "    def demand(self, j, i, t):\n",
    "        demand = np.round(\n",
    "            self.d_max[i-1]/2 +\n",
    "            self.d_max[i-1]/2*np.cos(4*np.pi*(2*j*i+t)/self.T) +\n",
    "            np.random.randint(0, self.d_var[i-1]+1))\n",
    "        return demand\n",
    "\n",
    "    def initial_state(self):\n",
    "        return State(self.product_types_num, self.distr_warehouses_num,\n",
    "                     self.T, list(self.demand_history))\n",
    "\n",
    "    def step(self, state, action):\n",
    "        demands = np.fromfunction(\n",
    "            lambda j, i: self.demand(j+1, i+1, self.t),\n",
    "            (self.distr_warehouses_num, self.product_types_num),\n",
    "            dtype=np.int32)\n",
    "\n",
    "        next_state = State(self.product_types_num, self.distr_warehouses_num,\n",
    "                          self.T, list(self.demand_history))\n",
    "\n",
    "        next_state.factory_stocks = np.minimum(\n",
    "            np.subtract(np.add(state.factory_stocks, action.production_level),\n",
    "                       np.sum(action.shipped_stocks, axis=0)),\n",
    "            self.storage_capacities[0])\n",
    "\n",
    "        for j in range(self.distr_warehouses_num):\n",
    "            next_state.distr_warehouses_stocks[j] = np.minimum(\n",
    "                np.subtract(np.add(state.distr_warehouses_stocks[j],\n",
    "                                  action.shipped_stocks[j]), demands[j]),\n",
    "                self.storage_capacities[j+1])\n",
    "\n",
    "        # Calculate reward\n",
    "        total_revenues = np.dot(self.sale_prices, np.sum(demands, axis=0))\n",
    "        total_production_costs = np.dot(self.production_costs, action.production_level)\n",
    "        total_transportation_costs = np.dot(\n",
    "            self.transportation_costs.flatten(), action.shipped_stocks.flatten())\n",
    "        total_storage_costs = np.dot(\n",
    "            self.storage_costs.flatten(),\n",
    "            np.maximum(next_state.stock_levels(),\n",
    "                      np.zeros(((self.distr_warehouses_num+1) * self.product_types_num),\n",
    "                              dtype=np.int32)))\n",
    "        total_penalty_costs = -np.dot(\n",
    "            self.penalty_costs,\n",
    "            np.add(np.sum(np.minimum(next_state.distr_warehouses_stocks,\n",
    "                                   np.zeros((self.distr_warehouses_num, self.product_types_num),\n",
    "                                           dtype=np.int32)), axis=0),\n",
    "                  np.minimum(next_state.factory_stocks,\n",
    "                           np.zeros((self.product_types_num,), dtype=np.int32))))\n",
    "\n",
    "        reward = (total_revenues - total_production_costs - \n",
    "                 total_transportation_costs - total_storage_costs - total_penalty_costs)\n",
    "\n",
    "        self.demand_history.append(demands)\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state, reward, self.t == self.T-1\n",
    "\n",
    "class SupplyChain(gym.Env):\n",
    "    \"\"\"Gym environment wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "        # Action space bounds\n",
    "        low_act = np.zeros(\n",
    "            ((self.supply_chain.distr_warehouses_num+1) * self.supply_chain.product_types_num),\n",
    "            dtype=np.int32)\n",
    "        high_act = np.zeros(\n",
    "            ((self.supply_chain.distr_warehouses_num+1) * self.supply_chain.product_types_num),\n",
    "            dtype=np.int32)\n",
    "        high_act[:self.supply_chain.product_types_num] = np.sum(\n",
    "            self.supply_chain.storage_capacities, axis=0)\n",
    "        high_act[self.supply_chain.product_types_num:] = (\n",
    "            self.supply_chain.storage_capacities.flatten()[self.supply_chain.product_types_num:])\n",
    "        \n",
    "        self.action_space = Box(low=low_act, high=high_act, dtype=np.int32)\n",
    "        \n",
    "        # Observation space bounds\n",
    "        low_obs = np.zeros((len(self.supply_chain.initial_state().to_array()),), dtype=np.int32)\n",
    "        low_obs[:self.supply_chain.product_types_num] = (\n",
    "            -np.sum(self.supply_chain.storage_capacities[1:], axis=0) * self.supply_chain.T)\n",
    "        low_obs[self.supply_chain.product_types_num:\n",
    "               (self.supply_chain.distr_warehouses_num+1) * self.supply_chain.product_types_num] = (\n",
    "            np.array([-(self.supply_chain.d_max+self.supply_chain.d_var) * self.supply_chain.T] * \n",
    "                    self.supply_chain.distr_warehouses_num).flatten())\n",
    "        \n",
    "        high_obs = np.zeros((len(self.supply_chain.initial_state().to_array()),), dtype=np.int32)\n",
    "        high_obs[:(self.supply_chain.distr_warehouses_num+1) * self.supply_chain.product_types_num] = (\n",
    "            self.supply_chain.storage_capacities.flatten())\n",
    "        from itertools import chain\n",
    "        high_obs[(self.supply_chain.distr_warehouses_num+1) * self.supply_chain.product_types_num:\n",
    "                len(high_obs)-1] = np.array([\n",
    "            self.supply_chain.d_max+self.supply_chain.d_var] * \n",
    "            len(list(chain(*self.supply_chain.demand_history)))).flatten()\n",
    "        high_obs[len(high_obs)-1] = self.supply_chain.T\n",
    "        \n",
    "        self.observation_space = Box(low=low_obs, high=high_obs, dtype=np.int32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.supply_chain = SupplyChainEnvironment()\n",
    "        self.state = self.supply_chain.initial_state()\n",
    "        return self.state.to_array()\n",
    "\n",
    "    def step(self, action):\n",
    "        action_obj = Action(\n",
    "            self.supply_chain.product_types_num,\n",
    "            self.supply_chain.distr_warehouses_num)\n",
    "        action_obj.production_level = action[:self.supply_chain.product_types_num].astype(np.int32)\n",
    "        action_obj.shipped_stocks = action[self.supply_chain.product_types_num:].reshape(\n",
    "            (self.supply_chain.distr_warehouses_num, self.supply_chain.product_types_num)).astype(np.int32)\n",
    "\n",
    "        self.state, reward, done = self.supply_chain.step(self.state, action_obj)\n",
    "        return self.state.to_array(), reward, done, {}\n",
    "\n",
    "# A3C Neural Network\n",
    "class A3CNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(A3CNetwork, self).__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_mean = nn.Linear(hidden_size, action_size)\n",
    "        self.actor_std = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        shared_features = self.shared(state)\n",
    "        \n",
    "        # Actor outputs\n",
    "        action_mean = self.actor_mean(shared_features)\n",
    "        action_std = F.softplus(self.actor_std(shared_features)) + 1e-5\n",
    "        \n",
    "        # Critic output\n",
    "        value = self.critic(shared_features)\n",
    "        \n",
    "        return action_mean, action_std, value\n",
    "\n",
    "# A3C Agent\n",
    "class A3CAgent:\n",
    "    def __init__(self, state_size, action_size, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.network = A3CNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_mean, action_std, _ = self.network(state)\n",
    "        \n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return action.squeeze().detach().numpy()\n",
    "    \n",
    "    def train(self, states, actions, rewards, next_states, dones):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        # Forward pass\n",
    "        action_means, action_stds, values = self.network(states)\n",
    "        _, _, next_values = self.network(next_states)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + self.gamma * R * (1 - dones[i])\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = returns - values.squeeze()\n",
    "        \n",
    "        # Actor loss\n",
    "        dist = Normal(action_means, action_stds)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "        # Optimization step\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "def train_a3c(env, episodes=1000, max_steps=25):\n",
    "    \"\"\"Train A3C agent on the supply chain environment\"\"\"\n",
    "    state_size = len(env.reset())\n",
    "    action_size = env.action_space.shape[0]\n",
    "    \n",
    "    agent = A3CAgent(state_size, action_size)\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Get action\n",
    "            action = agent.get_action(state)\n",
    "            # Clip action to valid range\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train agent\n",
    "        if len(states) > 0:\n",
    "            loss = agent.train(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    return agent, episode_rewards\n",
    "\n",
    "def simulate_episode(env, agent):\n",
    "    \"\"\"Simulate a single episode with trained agent\"\"\"\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(25):  # T = 25\n",
    "        action = agent.get_action(state)\n",
    "        action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return episode_reward, rewards\n",
    "\n",
    "def calculate_cum_profit(agent, env, num_episodes=200):\n",
    "    \"\"\"Calculate cumulative profit over multiple episodes\"\"\"\n",
    "    cumulative_profits = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode_reward, _ = simulate_episode(env, agent)\n",
    "        cumulative_profits.append(episode_reward)\n",
    "    \n",
    "    print(f\"Cumulative Profit - Mean: {np.mean(cumulative_profits):.2f}, \"\n",
    "          f\"Std: {np.std(cumulative_profits):.2f}, \"\n",
    "          f\"Max: {np.max(cumulative_profits):.2f}, \"\n",
    "          f\"Min: {np.min(cumulative_profits):.2f}\")\n",
    "    \n",
    "    return cumulative_profits\n",
    "\n",
    "def visualize_cum_profit(cumulative_profits, title=\"A3C Cumulative Profit\"):\n",
    "    \"\"\"Visualize cumulative profit distribution\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(cumulative_profits)\n",
    "    plt.ylabel('Cumulative Profit')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_progress(episode_rewards):\n",
    "    \"\"\"Plot training progress\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot episode rewards\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Episode Rewards During Training')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot moving average\n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = 50\n",
    "    moving_avg = [np.mean(episode_rewards[max(0, i-window):i+1]) \n",
    "                  for i in range(len(episode_rewards))]\n",
    "    plt.plot(moving_avg)\n",
    "    plt.title(f'Moving Average Reward (window={window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env = SupplyChain()\n",
    "    \n",
    "    # Train A3C agent\n",
    "    print(\"Training A3C agent...\")\n",
    "    agent, episode_rewards = train_a3c(env, episodes=1000)\n",
    "    \n",
    "    # Plot training progress\n",
    "    plot_training_progress(episode_rewards)\n",
    "    \n",
    "    # Evaluate trained agent\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    cumulative_profits = calculate_cum_profit(agent, env, num_episodes=200)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_cum_profit(cumulative_profits)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
